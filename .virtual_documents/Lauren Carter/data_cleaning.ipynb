#Data Cleaning
#This is the first step in our data analysis


# Import the required libraries and dependencies
import pandas as pd


# Define the path to the CSV file
data_path = "../Resources/HealthData.csv"


# Load the data without specifying dtype for problematic columns
df = pd.read_csv(
    data_path,
    dtype={
        'LocationAbbr': 'str',
        'LocationDesc': 'str',
        'GeographicLevel': 'str',
        'DataSource': 'str',
        'Class': 'str',
        'Topic': 'str',
        'Data_Value': 'float64',
        'Data_Value_Unit': 'str',
        'Data_Value_Type': 'str',
        'Data_Value_Footnote_Symbol': 'str',
        'Data_Value_Footnote': 'str',
        'Confidence_limit_Low': 'float64',
        'Confidence_limit_High': 'float64',
        'StratificationCategory1': 'str',
        'Stratification1': 'str',
        'StratificationCategory2': 'str',
        'Stratification2': 'str',
        'StratificationCategory3': 'str',
        'Stratification3': 'str',
        'LocationID': 'int64'  # Assuming it's an integer
    },
    low_memory=False
)


# Check for missing values
print(df.isnull().sum())


# Columns with known missing data that are critical for analysis
critical_columns_with_missing_data = [
    'Data_Value',
    'Confidence_limit_Low',
    'Confidence_limit_High'
]

# Filter out rows with missing critical data
df_filtered = df.dropna(subset=critical_columns_with_missing_data)

# Display the filtered DataFrame to verify
df_filtered


print(df_filtered.isnull().sum())


# Drop the columns with only missing values
columns_to_drop = ['Data_Value_Footnote_Symbol', 'Data_Value_Footnote']
df_cleaned = df_filtered.drop(columns=columns_to_drop)

# Drop the 'DataSource' column, not necessary for analysis 
df_cleaned = df_cleaned.drop(columns=['DataSource'])

# Display the cleaned DataFrame to verify
df_cleaned


# Convert the 'Year' column to numeric, handling errors
# Anaconda Assistant provided this error handling
df_cleaned.loc[:, 'Year'] = pd.to_numeric(df_cleaned['Year'], errors='coerce')

# Drop any rows where 'Year' could not be converted to a valid integer
df_cleaned = df_cleaned.dropna(subset=['Year'])

# Convert 'Year' column to integer type
df_cleaned.loc[:, 'Year'] = df_cleaned['Year'].astype(int)

# Filter the dataset to include only rows from the most recent years (2010-2019)
df_recent_years = df_cleaned[df_cleaned['Year'] >= 2010]

# Check the number of rows in the filtered dataset
print(f"The filtered DataFrame (2010-2019) has {len(df_recent_years)} rows.")


# Check all columns in the DataFrame for unique values
for column in df_recent_years.columns:
    unique_values = df_recent_years[column].unique()
    if len(unique_values) == 1:
        print(f"The '{column}' column has the same value across all rows.")
        print(f"The single value is: {unique_values[0]}")


# List of columns that have the same value across all rows and should be dropped
columns_to_drop = [
    'GeographicLevel', 
    'Class', 
    'Data_Value_Unit', 
    'Data_Value_Type',
    'StratificationCategory1',
    'StratificationCategory2',
    'StratificationCategory3'
]

# Drop the specified columns
df_recent_years = df_recent_years.drop(columns=columns_to_drop)

# Verify the columns have been dropped
print("Remaining columns after dropping non-unique columns:")
print(df_recent_years.columns)


# Check for duplicates
duplicate_rows = df_recent_years.duplicated().sum()
print(f"Number of duplicate rows: {duplicate_rows}")


# Example: Identifying potential outliers in 'Data_Value'
df_recent_years['Data_Value'].describe()


# Filtering out extreme outliers
lower_bound = df_recent_years['Data_Value'].quantile(0.05)
upper_bound = df_recent_years['Data_Value'].quantile(0.95)

df_recent_years = df_recent_years[(df_recent_years['Data_Value'] >= lower_bound) & 
                                  (df_recent_years['Data_Value'] <= upper_bound)]
print(f"After outlier removal, the DataFrame has {len(df_recent_years)} rows.")


print(df_recent_years.dtypes)


# Rename the columns
df_recent_years = df_recent_years.rename(columns={
    'LocationAbbr': 'State',
    'LocationDesc': 'County',
    'Topic': 'CauseOfDeath',
    'Data_Value': 'MortalityRate',  # Assuming this represents the rate per 100,000
    'Confidence_limit_Low': 'ConfLow',
    'Confidence_limit_High': 'ConfHigh',
    'Stratification1': 'AgeGroup',
    'Stratification2': 'Race',
    'Stratification3': 'Sex'
})

# Verify the column renaming
print("Columns after renaming:")
print(df_recent_years.columns)



# Display the final cleaned DataFrame
df_recent_years


# Save the cleaned DataFrame to a new CSV file
df_recent_years.to_csv('Cleaned_HealthData.csv', index=False)
